{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b735b9f5-391b-4db8-984d-2bb3a296309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zad11 import bench, connect, load_table, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a9c494-e6a1-4ffa-afc2-2bbc4166600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/09 07:10:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/09 07:10:12 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-lab:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Zad11</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f28d036c8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8cf67e1-f514-4b36-a02b-09dec420d0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading '/input/daily_weather_2017.csv' into table 'daily_weather_2017'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "daily_weather_2017 = load_table(spark, \"/input/daily_weather_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c05a48d-b0a0-48cf-b4f4-dcbeca8b2ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading '/input/cities.csv' into table 'cities'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cities = load_table(spark, \"/input/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9eabd9d-55fd-4216-bd08-770a9d5164bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+----------------+\n",
      "|    country|      date|       temperature_c|precipitation_mm|\n",
      "+-----------+----------+--------------------+----------------+\n",
      "|Afghanistan|2017-01-01|  5.3166666666666655|             0.0|\n",
      "|Afghanistan|2017-01-02|   5.016666666666667|             2.0|\n",
      "|Afghanistan|2017-01-03|  3.0666666666666664|          10.725|\n",
      "|Afghanistan|2017-01-04|                2.65|           109.0|\n",
      "|Afghanistan|2017-01-05|  1.9333333333333333|           29.95|\n",
      "|Afghanistan|2017-01-06|  0.9833333333333331|            7.25|\n",
      "|Afghanistan|2017-01-07|  0.2833333333333334|            33.0|\n",
      "|Afghanistan|2017-01-08|-0.07999999999999999|            18.0|\n",
      "|Afghanistan|2017-01-09|-0.21666666666666665|             0.0|\n",
      "|Afghanistan|2017-01-10|   0.866666666666667|             0.0|\n",
      "|Afghanistan|2017-01-11| 0.44999999999999973|             0.0|\n",
      "|Afghanistan|2017-01-12| -0.6166666666666668|             0.0|\n",
      "|Afghanistan|2017-01-13| -0.8399999999999996|             0.0|\n",
      "|Afghanistan|2017-01-14|               -0.75|             9.9|\n",
      "|Afghanistan|2017-01-15| -2.1333333333333333|             3.8|\n",
      "|Afghanistan|2017-01-16| -0.5166666666666665|             0.0|\n",
      "|Afghanistan|2017-01-17|-0.06666666666666665|            18.0|\n",
      "|Afghanistan|2017-01-18|-0.11666666666666654|             1.0|\n",
      "|Afghanistan|2017-01-19|  2.6333333333333333|           29.95|\n",
      "|Afghanistan|2017-01-20|                 2.2|             0.0|\n",
      "+-----------+----------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time: 7.020 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col, avg, coalesce, lit\n",
    "\n",
    "result_df = (\n",
    "    (\n",
    "        (\n",
    "            daily_weather_2017.filter(\n",
    "                (col(\"date\") >= \"2017-01-01\") & (col(\"date\") <= \"2021-12-31\")\n",
    "            ).select(\n",
    "                col(\"station_id\"),\n",
    "                to_date(col(\"date\")).alias(\"date\"),\n",
    "                col(\"avg_temp_c\").alias(\"temperature_c\"),\n",
    "                col(\"precipitation_mm\"),\n",
    "            )\n",
    "        )\n",
    "        .join(cities, on=\"station_id\")\n",
    "        .groupBy(\"country\", \"date\")\n",
    "        .agg(\n",
    "            avg(\"temperature_c\").alias(\"temperature_c\"),\n",
    "            coalesce(avg(\"precipitation_mm\"), lit(0)).alias(\"precipitation_mm\"),\n",
    "        )\n",
    "    )\n",
    "    .filter(col(\"temperature_c\").isNotNull())\n",
    "    .orderBy(\"country\", \"date\")\n",
    ")\n",
    "\n",
    "\n",
    "with bench():\n",
    "    result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f9d670-7be2-4d94-b13c-190207f840e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
